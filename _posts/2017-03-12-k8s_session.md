---
tags: container
layout: post
title: Kubernetes session phase1
category: orchestration
---
为什么会考虑k8s? 我们能获得什么?<!--more-->

### 背景介绍
　　云计算飞速发展 <br/>
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; - IaaS<br/>
　　　　- PaaS<br/>
　　　　- SaaS<br/>
　　Docker技术突飞猛进<br/>
　　　　- 一次构建，到处运行<br/>
　　　　- 容器的快速轻量<br/>
　　　　- 完整的生态环境<br/>
### 什么是kubernetes
　　Kubernetes(k8s)是Google开源的容器集群管理系统（谷歌内部:Borg）。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。<br/>
　　Kubernetes优势: <br/>
　　　　- 容器编排<br/>
　　　　- 轻量级<br/>
　　　　- 开源<br/>
　　　　- 弹性伸缩<br/>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; - 负载均衡<br/>


# k8s Architecture

A running Kubernetes cluster contains node agents (`kubelet`) and master
components (APIs, scheduler, etc), on top of a distributed storage solution.
This diagram shows our desired eventual state, though we're still working on a
few things, like making `kubelet` itself (all our components, really) run within
containers, and making the scheduler 100% pluggable.


![Architecture Diagram](http://images2015.cnblogs.com/blog/937245/201612/937245-20161212183648198-2021561046.png "Architecture overview")

## The Kubernetes Control Plane (Master节点)

The Kubernetes control plane is split into a set of components. Currently they
all run on a single _master_ node, but that is expected to change soon in order
to support high-availability clusters. These components work together to provide
a unified view of the cluster.

### `etcd`

All persistent master state is stored in an instance of `etcd`. This provides a
great way to store configuration data reliably. With `watch` support,
coordinating components can be notified very quickly of changes.

### `API Server`

API Server作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTful接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd。The apiserver serves up the [Kubernetes API](../api.md). It is intended to be a
CRUD-y server, with most/all business logic implemented in separate components
or in plug-ins. It mainly processes REST operations, validates them, and updates
the corresponding objects in `etcd` (and eventually other stores).

### `Scheduler`

scheduler的作用是将待调度的pod（API新创建的Pod、Controller Manager为补足副本而创建的pod等）按照特定的调度算法和调度策略绑定（binding）到集群中的某个合适的node上，并将绑定信息写入etcd中。在整个调度过程中涉及三个对象，分别是：待调度的pod列表、可用node列表、以及调度算法和策略。简单地说，就是通过调度算法调度，为待调度pod列表中的每个pod从node列表中选择一个最适合的node。

关键对象分析：
[`SchedulerServer`](https://github.com/kubernetes/kubernetes/blob/master/plugin/cmd/kube-scheduler/app/options/options.go#L37)
[`KubeSchedulerConfiguration`](https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/componentconfig/types.go#L566)
[`Schedule interface`](https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/scheduler/algorithm/scheduler_interface.go#L42)
[algorithm implementation by default](https://github.com/kubernetes/kubernetes/blob/aaaa7e4425dd28606178a90549943c5c47d394fa/plugin/pkg/scheduler/algorithmprovider/defaults/defaults.go#L60)

### `Controller Manager`

负责管理各种控制器，目前有(pluggable)：

    - Endpoint Controller：关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。
    - Replication Controller：关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。
    - Node controller: Node Controller负责发现、管理和监控集群中的各个node节点。Kubelet在启动时通过API Server注册节点信息，并定时向API Server发送节点信息。API Server接收到这些信息后，将这些信息写入etcd。存入etcd的节点信息包括节点健康状况、节点资源、节点名称、节点地址信息、操作系统版本、docker版本、kubelet版本等。
[detail](https://github.com/kubernetes/kubernetes/tree/7585c46bc18913232a72d83e2f5a02a5a1f65c1b/pkg/controller)


## The Kubernetes Node (Worker节点)

node是kubernetes集群中相对于master而言的工作主机，在较早版本中也被称为minion。Node可以是一台物理主机，也可以是一台虚拟机（VM）。在每个node上运行用于启动和管理pod的服务——kubelet，并能够被master管理。在node上运行的服务进程包括kubelet、kube-proxy和docker daemon。
Node的信息如下：
node地址：主机的IP地址或者nodeid
node的运行状态: pending,running,terminated
node condition: 描述running状态node的运行条件，目前只有一种条件Ready，表示node处于健康状态，可以接收master发来的创建pod的指令。
node系统容量：描述node可用的系统资源，包括CPU、内存、最大可调度pod数量等

### `kubelet`

负责具体管理docker容器相关操作，如下载镜像，启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的[pods](https://kubernetes.io/docs/user-guide/pods.md)，并根据pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报pod的运行状态。

### `kube-proxy`

kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。

举个例子，现在有podA，podB，podC和serviceAB。serviceAB是podA，podB的服务抽象(service)。
那么kube-proxy的作用就是可以将pod(不管是podA，podB或者podC)向serviceAB的请求，进行转发到service所代表的一个具体pod(podA或者podB)上。
请求的分配方法一般分配是采用轮询方法进行分配。

kuer-proxy目前有userspace和iptables两种实现方式。
userspace是在用户空间，通过kuber-proxy实现LB的代理服务。这个是kube-proxy的最初的版本，较为稳定，但是效率也自然不太高。
另外一种方式是iptables的方式。是纯采用iptables来实现LB。是目前一般kube默认的方式。

[kube-proxy内部原理](https://kubernetes.io/docs/user-guide/services/#proxy-mode-userspace)

# Core concepts

## `Pods`

一个Pod对应于由若干容器组成的一个容器组，同个组内的容器共享一个存储卷(volume)。Pod主要是在容器化环境中建立了一个面向应用的“逻辑主机”模型，它可以包含一个或多个相互间紧密联系的容器。在没有容器化技术的场景里，同个Pod内的“容器”都在同一台物理或虚拟主机上运行。

Pod与容器一样都不是持续存在的，在容器的生命周期里，每个Pod被分配到节点上运行直至运行结束或被删除。当一个节点消失时，该节点上的Pod也随之被删除。每个Pod实体只会被调度一次，不会重复分配给别的节点，由replication controller负责创建新的Pod来替代旧的（在未来也可能有新的API用于Pod迁移）。<br/>
<img src="https://kubernetes.io/images/docs/pod.svg" width = "25%" />

推荐使用方式Deployment/RCs

## `Networking`
本身只提供一种模型[CNI](https://github.com/containernetworking/cni),完全依赖第三方，比如flannel <br/>
[私有IP分配规范/RFC1918](https://tools.ietf.org/html/rfc1918)<br/>
[Docker model](https://github.com/docker/libnetwork/blob/master/docs/design.md)

## `Volume`
存储:<br/>
大家都知道容器本身一般不会对数据进行持久化处理，在Kubernetes中，容器异常退出，kubelet也只是简单的基于原有镜像重启一个新的容器。另外，如果我们在同一个Pod中运行多个容器，经常会需要在这些容器之间进行共享一些数据。Kuberenetes 的 Volume就是主要来解决上面两个基础问题的。<br/>
`linux mount` directory to path <br/>
volume type [FS] <br/>

[PV](https://kubernetes.io/docs/user-guide/persistent-volumes/): A PersistentVolume (PV) is a piece of networked storage in the cluster that has been provisioned by an administrator. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual pod that uses the PV.


## `Labels`

标签其实就一对 key/value ，被关联到对象上，比如Pod,标签的使用我们倾向于能够标示对象的特殊特点，并且对用户而言是有意义的（就是一眼就看出了这个Pod是尼玛数据库），但是标签对内核系统是没有直接意义的。标签可以用来划分特定组的对象（比如，所有女的），标签可以在创建一个对象的时候直接给与，也可以在后期随时修改，每一个对象可以拥有多个标签，但是，key值必须是唯一的

```json
"labels": {
  "key1" : "value1",
  "key2" : "value2"
}
```

我们最终会索引并且反向索引（reverse-index）labels，以获得更高效的查询和监视，把他们用到UI或者CLI中用来排序或者分组等等。我们不想用那些不具有指认效果的label来污染label，特别是那些体积较大和结构型的的数据。不具有指认效果的信息应该使用annotation来记录。

Example labels:

   * `"release" : "stable"`, `"release" : "canary"`
   * `"environment" : "dev"`, `"environment" : "qa"`, `"environment" : "production"`
   * `"tier" : "frontend"`, `"tier" : "backend"`, `"tier" : "cache"`
   * `"partition" : "customerA"`, `"partition" : "customerB"`
   * `"track" : "daily"`, `"track" : "weekly"`

These are just examples; you are free to develop your own conventions.

### `Selectors`
The API currently supports two types of selectors: equality-based and set-based.
```shell
$ kubectl get pods -l environment=production,tier=frontend
```
or using _set-based_ requirements:

```shell
$ kubectl get pods -l 'environment in (production, qa),tier in (frontend)'
```

## `Services`

在kubernetes的世界里，虽然每个pod都会被分配一个单独的IP地址，但这个IP地址会随着pod的销毁而消失。这就引出一个问题：如果有一组pod组成一个集群来提供服务，那么如何来访问它们呢？
kubernetes的service就是用来解决这个问题的核心概念。一个service可以看作一组提供相同服务的pod的对外访问接口。Service作用于哪些pod是通过label selector 来定义的。
pod的IP地址是docker daemon根据docker0网桥的IP地址段进行分配的，但service的Cluster IP地址是kubernetes系统中的虚拟IP地址，由系统动态分配。 Service的ClusterIP地址相对于pod的IP地址来说相对稳定，service被创建时即被分配IP地址，在销毁该service之前，这个IP地址都不会再变化。
由于service对象在Cluster IP Range池中分配到的IP只能在内部访问，所以其他pod都可以无障碍地访问到它。但如果这个service作为前端服务，准备为集群外的客户端提供服务，我们就需要给这个服务提供公共IP了

![Services abstraction diagram](http://img1.tuicool.com/2qQNZnU.jpg!web)

从Service本身看，有三种方式来暴露访问：<br/>

    ClusterIP：使用集群内的私有ip —— 这是默认值

    NodePort：除了使用cluster ip外，也将service的port映射到每个node的一个指定内部port上，映射的每个node的内部port都一样。

    LoadBalancer：使用一个ClusterIP & NodePort，但是会向cloud provider申请映射到service本身的负载均衡。


<!--![Services overview diagram for iptables proxy](http://img0.tuicool.com/jEjqUjF.png!web)-->

[For more](https://kubernetes.io/docs/concepts/)